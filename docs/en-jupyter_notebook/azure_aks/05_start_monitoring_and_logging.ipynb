{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 start monitoring & logging on Azure AKS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "change ${PJ_ROOT} to your directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export PJ_ROOT=\"${HOME}/core\"\n",
    "cd ${PJ_ROOT};pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example)\n",
    "```\n",
    "/Users/user/core\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source ${PJ_ROOT}/docs/environments/azure_aks/env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup alias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if [ \"$(uname)\" == 'Darwin' ]; then\n",
    "  alias openbrowser='open'\n",
    "elif [ \"$(expr substr $(uname -s) 1 5)\" == 'Linux' ]; then\n",
    "  alias openbrowser='xdg-open'\n",
    "else\n",
    "  echo \"Your platform ($(uname -a)) is not supported.\"\n",
    "  exit 1\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## start fiware cygnus for elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl apply -f cygnus/cygnus-elasticsearch-service.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl apply -f cygnus/cygnus-elasticsearch-deployment.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get pods -l app=cygnus-elasticsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example)\n",
    "```\n",
    "NAME                                    READY   STATUS    RESTARTS   AGE\n",
    "cygnus-elasticsearch-689b7f5fd8-dtptx   1/1     Running   0          36s\n",
    "cygnus-elasticsearch-689b7f5fd8-wj5vm   1/1     Running   0          36s\n",
    "cygnus-elasticsearch-689b7f5fd8-xnhhj   1/1     Running   0          36s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get services -l app=cygnus-elasticsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example)\n",
    "```\n",
    "NAME                   TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)             AGE\n",
    "cygnus-elasticsearch   ClusterIP   10.0.93.83   <none>        5050/TCP,8081/TCP   1m\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## start prometheus & grafana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### install prometheus-operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "helm install stable/prometheus-operator --name po --namespace monitoring -f monitoring/prometheus-operator-azure.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kubectl --namespace monitoring get pods -l \"app=prometheus-operator-operator,release=po\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example)\n",
    "```\n",
    "NAME                                               READY   STATUS    RESTARTS   AGE\n",
    "po-prometheus-operator-operator-7cf7c5cc97-78h9g   1/1     Running   0          2m28s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get daemonsets --namespace monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example)\n",
    "```\n",
    "NAME                          DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE\n",
    "po-prometheus-node-exporter   4         4         4       4            4           <none>          2m48s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get deployments --namespace monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example)\n",
    "```\n",
    "NAME                              READY   UP-TO-DATE   AVAILABLE   AGE\n",
    "po-grafana                        1/1     1            1           3m51s\n",
    "po-kube-state-metrics             1/1     1            1           3m51s\n",
    "po-prometheus-operator-operator   1/1     1            1           3m51s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get statefulsets --namespace monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example)\n",
    "```\n",
    "NAME                                               READY   AGE\n",
    "alertmanager-po-prometheus-operator-alertmanager   1/1     3m44s\n",
    "prometheus-po-prometheus-operator-prometheus       1/1     3m34s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get pods --namespace monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example)\n",
    "```\n",
    "NAME                                                 READY   STATUS    RESTARTS   AGE\n",
    "alertmanager-po-prometheus-operator-alertmanager-0   2/2     Running   0          3m56s\n",
    "po-grafana-fbc85bc4b-5k2s8                           2/2     Running   0          4m32s\n",
    "po-kube-state-metrics-64fdf7d84d-v9d8h               1/1     Running   0          4m32s\n",
    "po-prometheus-node-exporter-4fptr                    1/1     Running   0          4m32s\n",
    "po-prometheus-node-exporter-92lzp                    1/1     Running   0          4m32s\n",
    "po-prometheus-node-exporter-h8hff                    1/1     Running   0          4m32s\n",
    "po-prometheus-node-exporter-76pc4                    1/1     Running   0          4m32s\n",
    "po-prometheus-operator-operator-7cf7c5cc97-78h9g     1/1     Running   0          4m32s\n",
    "prometheus-po-prometheus-operator-prometheus-0       3/3     Running   1          3m46s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get services --namespace monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example)\n",
    "```\n",
    "NAME                                  TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)             AGE\n",
    "alertmanager-operated                 ClusterIP   None           <none>        9093/TCP,6783/TCP   6m4s\n",
    "po-grafana                            ClusterIP   10.0.67.251    <none>        80/TCP              6m40s\n",
    "po-kube-state-metrics                 ClusterIP   10.0.104.75    <none>        8080/TCP            6m40s\n",
    "po-prometheus-node-exporter           ClusterIP   10.0.103.93    <none>        9100/TCP            6m40s\n",
    "po-prometheus-operator-alertmanager   ClusterIP   10.0.57.200    <none>        9093/TCP            6m40s\n",
    "po-prometheus-operator-operator       ClusterIP   10.0.37.49     <none>        8080/TCP            6m40s\n",
    "po-prometheus-operator-prometheus     ClusterIP   10.0.229.252   <none>        9090/TCP            6m40s\n",
    "prometheus-operated                   ClusterIP   None           <none>        9090/TCP            5m54s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get persistentvolumeclaims --namespace monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example)\n",
    "```\n",
    "NAME                                                                                       STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS      AGE\n",
    "prometheus-alertmanager-storage-claim-alertmanager-po-prometheus-operator-alertmanager-0   Bound    pvc-82a98fe2-9f94-11e9-8e06-7200ae7cb77a   30Gi       RWO            managed-premium   119s\n",
    "prometheus-prometheus-storage-claim-prometheus-po-prometheus-operator-prometheus-0         Bound    pvc-88b5edbb-9f94-11e9-8e06-7200ae7cb77a   30Gi       RWO            managed-premium   109s\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### edit some prometheus rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "echo 'kubectl edit --namespace=monitoring prometheusrules po-prometheus-operator-general.rules'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```diff\n",
    "       for: 10m\n",
    "       labels:\n",
    "         severity: warning\n",
    "-    - alert: Watchdog\n",
    "-      annotations:\n",
    "-        message: |\n",
    "-          This is an alert meant to ensure that the entire alerting pipeline is functional.\n",
    "-          This alert is always firing, therefore it should always be firing in Alertmanager\n",
    "-          and always fire against a receiver. There are integrations with various notification\n",
    "-          mechanisms that send a notification when this alert is not firing. For example the\n",
    "-          \"DeadMansSnitch\" integration in PagerDuty.\n",
    "-      expr: vector(1)\n",
    "-      labels:\n",
    "-        severity: none\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "echo 'kubectl edit --namespace=monitoring prometheusrules po-prometheus-operator-kubernetes-absent'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```diff\n",
    "       for: 15m\n",
    "       labels:\n",
    "         severity: critical\n",
    "-    - alert: KubeControllerManagerDown\n",
    "-      annotations:\n",
    "-        message: KubeControllerManager has disappeared from Prometheus target discovery.\n",
    "-        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubecontrollermanagerdown\n",
    "-      expr: absent(up{job=\"kube-controller-manager\"} == 1)\n",
    "-      for: 15m\n",
    "-      labels:\n",
    "-        severity: critical\n",
    "-    - alert: KubeSchedulerDown\n",
    "-      annotations:\n",
    "-        message: KubeScheduler has disappeared from Prometheus target discovery.\n",
    "-        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeschedulerdown\n",
    "-      expr: absent(up{job=\"kube-scheduler\"} == 1)\n",
    "-      for: 15m\n",
    "-      labels:\n",
    "-        severity: critical\n",
    "     - alert: KubeStateMetricsDown\n",
    "       annotations:\n",
    "         message: KubeStateMetrics has disappeared from Prometheus target discovery.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### confirm prometheus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "echo 'kubectl --namespace monitoring port-forward $(kubectl get pod --namespace monitoring -l prometheus=kube-prometheus -l app=prometheus -o template --template \"{{(index .items 0).metadata.name}}\") 9090:9090'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openbrowser http://localhost:9090"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. confirm Prometheus\n",
    "    * no `Target` is down.\n",
    "    * no `Alert` is fired except CPU or Memory resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### patch grafana service\n",
    "* add the \"annotation\" of ambassador in order to access from Internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl patch service --namespace monitoring po-grafana -p '{\"metadata\": {\"annotations\": {\"getambassador.io/config\": \"---\\napiVersion: ambassador/v0\\nkind:  Mapping\\nname:  grafana-mapping\\nprefix: /\\nhost: \\\"^grafana\\\\\\\\..+$\\\"\\nhost_regex: true\\nservice: http://po-grafana.monitoring:80\\n\"}}}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### register DNS A Record for grafana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTTPS_IPADDR=$(kubectl get services -l app=ambassador -o json | jq '.items[0].status.loadBalancer.ingress[0].ip' -r)\n",
    "az network dns record-set a add-record --resource-group ${DNS_ZONE_RG} --zone-name \"${DOMAIN}\" --record-set-name \"grafana\" --ipv4-address \"${HTTPS_IPADDR}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setup grafana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openbrowser https://grafana.${DOMAIN}/login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. login grafana\n",
    "    * At the first, a admin user (`admin`/`prom-operator`) is available.\n",
    "2. change the admin's password"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## start Elasticsearch, fluentd and Kibana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### start Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl apply -f logging/elasticsearch-azure-service.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl apply -f logging/elasticsearch-azure-deployment.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get statefulsets --namespace monitoring -l k8s-app=elasticsearch-logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example)\n",
    "```\n",
    "NAME                    READY   AGE\n",
    "elasticsearch-logging   2/2     5m18s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get pods --namespace monitoring -l k8s-app=elasticsearch-logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example)\n",
    "```\n",
    "NAME                      READY     STATUS    RESTARTS   AGE\n",
    "elasticsearch-logging-0   1/1       Running   0          4m\n",
    "elasticsearch-logging-1   1/1       Running   0          2m\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get services --namespace monitoring -l k8s-app=elasticsearch-logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example)\n",
    "```\n",
    "NAME                    TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)    AGE\n",
    "elasticsearch-logging   ClusterIP   10.0.80.88   <none>        9200/TCP   4m\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get persistentvolumeclaims -n monitoring -l k8s-app=elasticsearch-logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example)\n",
    "```\n",
    "NAME                                            STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS      AGE\n",
    "elasticsearch-logging-elasticsearch-logging-0   Bound     pvc-238139db-b014-11e8-b618-066567bdfa8c   64Gi       RWO            managed-premium   4m\n",
    "elasticsearch-logging-elasticsearch-logging-1   Bound     pvc-70ca5ec3-b014-11e8-b618-066567bdfa8c   64Gi       RWO            managed-premium   2m\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl exec -it elasticsearch-logging-0 --namespace monitoring -- curl -H \"Content-Type: application/json\" -X PUT http://elasticsearch-logging:9200/_cluster/settings -d '{\"transient\": {\"cluster.routing.allocation.enable\":\"all\"}}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### start fluentd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl apply -f logging/fluentd-es-configmap.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl apply -f logging/fluentd-es-ds.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get daemonsets --namespace monitoring -l k8s-app=fluentd-es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example)\n",
    "```\n",
    "NAME                DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE\n",
    "fluentd-es-v2.2.0   4         4         4       4            4           <none>          53s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get pods --namespace monitoring -l k8s-app=fluentd-es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example)\n",
    "```\n",
    "NAME                      READY   STATUS    RESTARTS   AGE\n",
    "fluentd-es-v2.2.0-8sv45   1/1     Running   0          1m\n",
    "fluentd-es-v2.2.0-96ghs   1/1     Running   0          1m\n",
    "fluentd-es-v2.2.0-cjhtc   1/1     Running   0          1m\n",
    "fluentd-es-v2.2.0-djzff   1/1     Running   0          1m\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### start Kibana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl apply -f logging/kibana-service.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl apply -f logging/kibana-deployment.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get pods --namespace monitoring -l k8s-app=kibana-logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example)\n",
    "```\n",
    "NAME                              READY     STATUS    RESTARTS   AGE\n",
    "kibana-logging-7444956bf8-stnfm   1/1       Running   0          1m\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### register DNS A Record for Kibana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTTPS_IPADDR=$(kubectl get services -l app=ambassador -o json | jq '.items[0].status.loadBalancer.ingress[0].ip' -r)\n",
    "az network dns record-set a add-record --resource-group ${DNS_ZONE_RG} --zone-name \"${DOMAIN}\" --record-set-name \"kibana\" --ipv4-address \"${HTTPS_IPADDR}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### start curator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl apply -f logging/curator-configmap.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl apply -f logging/curator-cronjob.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get cronjobs --namespace monitoring -l k8s-app=elasticsearch-curator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example)\n",
    "```\n",
    "NAME                    SCHEDULE     SUSPEND   ACTIVE    LAST SCHEDULE   AGE\n",
    "elasticsearch-curator   0 18 * * *   False     0         <none>          7s\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### confirm basic auth username & password for Kibana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat ${PJ_ROOT}/secrets/auth-tokens.json | jq '.[]|select(.host == \"kibana\\\\..+$\")|.settings.basic_auths[0].username' -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat ${PJ_ROOT}/secrets/auth-tokens.json | jq '.[]|select(.host == \"kibana\\\\..+$\")|.settings.basic_auths[0].password' -r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setup Kibana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openbrowser https://kibana.${DOMAIN}/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Login kibana by basic authorization using above username and password\n",
    "1. show `Management -> Index Patterns`\n",
    "2. set `logstash-*` as Index Pattern, and push `Next step`\n",
    "3. set `@timestamp` as Time Filter field name, and push `Create index pattern`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setup grafana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openbrowser https://grafana.${DOMAIN}/login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add `elasticsearch` dashboard to grafana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. add a new Data Source (ElasticSearch)\n",
    "    * Name: `elasticsearch`\n",
    "    * URL: `http://elasticsearch-logging:9200`\n",
    "    * Access: `Server(Default)`\n",
    "    * Index name: `logstash-*`\n",
    "    * Time field name: `@timestamp`\n",
    "    * Version: `6.0+`\n",
    "2. import `monitoring/dashboard_elasticsearch.json`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
