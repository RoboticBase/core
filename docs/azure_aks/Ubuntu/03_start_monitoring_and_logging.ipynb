{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 start monitoring & logging on Azure AKS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "change ${PJ_ROOT} to your directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export PJ_ROOT=\"${HOME}/core\"\n",
    "cd ${PJ_ROOT};pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example)\n",
    "```\n",
    "/Users/user/roboticbase-core\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source ${PJ_ROOT}/docs/azure_aks/env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## start fiware cygnus for elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl apply -f cygnus/cygnus-elasticsearch.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get pods -l app=cygnus-elasticsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example)\n",
    "```\n",
    "NAME                                    READY   STATUS    RESTARTS   AGE\n",
    "cygnus-elasticsearch-689b7f5fd8-dtptx   1/1     Running   0          36s\n",
    "cygnus-elasticsearch-689b7f5fd8-wj5vm   1/1     Running   0          36s\n",
    "cygnus-elasticsearch-689b7f5fd8-xnhhj   1/1     Running   0          36s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get services -l app=cygnus-elasticsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example)\n",
    "```\n",
    "NAME                   TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)             AGE\n",
    "cygnus-elasticsearch   ClusterIP   10.0.93.83   <none>        5050/TCP,8081/TCP   1m\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## start prometheus & grafana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### install coreos/prometheus-operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helm repo add coreos https://s3-eu-west-1.amazonaws.com/coreos-charts/stable/\n",
    "helm install coreos/prometheus-operator --name po --namespace monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl --namespace monitoring get pods -l \"app=prometheus-operator,release=po\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example)\n",
    "```\n",
    "NAME                                      READY     STATUS    RESTARTS   AGE\n",
    "po-prometheus-operator-7f75b4645b-xznff   1/1       Running   0          3m\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### install coreos/kube-prometheus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "helm install coreos/kube-prometheus --name kp --namespace monitoring -f monitoring/kube-prometheus-azure.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get daemonsets --namespace monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example)\n",
    "```\n",
    "NAME               DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE\n",
    "kp-exporter-node   4         4         4       4            4           <none>          11s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get deployments --namespace monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example)\n",
    "```\n",
    "NAME                     DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\n",
    "kp-exporter-kube-state   1         1         1            1           54s\n",
    "kp-grafana               1         1         1            1           54s\n",
    "po-prometheus-operator   1         1         1            1           3m\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get statefulsets --namespace monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example)\n",
    "```\n",
    "NAME                       DESIRED   CURRENT   AGE\n",
    "alertmanager-kp            1         1         1m\n",
    "prometheus-kp-prometheus   1         1         1m\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get pods --namespace monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example)\n",
    "```\n",
    "NAME                                     READY     STATUS    RESTARTS   AGE\n",
    "alertmanager-kp-0                        2/2       Running   0          2m\n",
    "kp-exporter-kube-state-89bc454b9-m75pz   2/2       Running   0          1m\n",
    "kp-exporter-node-cvsxc                   1/1       Running   0          2m\n",
    "kp-exporter-node-m888f                   1/1       Running   0          2m\n",
    "kp-exporter-node-rldvr                   1/1       Running   0          2m\n",
    "kp-grafana-74dff5b954-b8kvr              2/2       Running   0          2m\n",
    "po-prometheus-operator-78c74bd9f-4tdht   1/1       Running   0          4m\n",
    "prometheus-kp-prometheus-0               3/3       Running   1          2m\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get services --namespace monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example)\n",
    "```\n",
    "NAME                     TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)             AGE\n",
    "alertmanager-operated    ClusterIP   None           <none>        9093/TCP,6783/TCP   2m\n",
    "kp-alertmanager          ClusterIP   10.0.220.192   <none>        9093/TCP            2m\n",
    "kp-exporter-kube-state   ClusterIP   10.0.116.154   <none>        80/TCP              2m\n",
    "kp-exporter-node         ClusterIP   10.0.173.208   <none>        9100/TCP            2m\n",
    "kp-grafana               ClusterIP   10.0.7.29      <none>        80/TCP              2m\n",
    "kp-prometheus            ClusterIP   10.0.137.247   <none>        9090/TCP            2m\n",
    "prometheus-operated      ClusterIP   None           <none>        9090/TCP            2m\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get persistentvolumeclaims --namespace monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example)\n",
    "```\n",
    "NAME                                                     STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS      AGE\n",
    "alertmanager-kp-db-alertmanager-kp-0                     Bound     pvc-95d5a26c-b010-11e8-b618-066567bdfa8c   30Gi       RWO            managed-premium   3m\n",
    "prometheus-kp-prometheus-db-prometheus-kp-prometheus-0   Bound     pvc-95f599bb-b010-11e8-b618-066567bdfa8c   30Gi       RWO            managed-premium   3m\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### patch kube-dns-v20\n",
    "* Azure AKS does not export dns metrics\n",
    "    * https://github.com/Azure/AKS/issues/345"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl patch deployment --namespace kube-system kube-dns-v20 --patch \"$(cat monitoring/kube-dns-azure-patch.yaml)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### patch kube-prometheus-exporter-kubelets\n",
    "* the ServiceMonitor of kubelets on Azur AKS does not accept https\n",
    "    * https://github.com/coreos/prometheus-operator/issues/926"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get servicemonitor --namespace monitoring kp-exporter-kubelets -o yaml | sed 's/https/http/' | kubectl replace -f -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### delete ServiceMonitor of apiserver\n",
    "* the ServiceMonitor of apiserver on Azure AKS does not allow to connect directry\n",
    "    * https://github.com/coreos/prometheus-operator/issues/1522"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl delete servicemonitor --namespace monitoring kp-exporter-kubernetes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### edit some prometheus rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "echo 'kubectl edit prometheusrules --namespace monitoring kp-kube-prometheus'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```diff\n",
    "       for: 10m\n",
    "       labels:\n",
    "         severity: warning\n",
    "-    - alert: DeadMansSwitch\n",
    "-      annotations:\n",
    "-        description: This is a DeadMansSwitch meant to ensure that the entire Alerting\n",
    "-          pipeline is functional.\n",
    "-        summary: Alerting DeadMansSwitch\n",
    "-      expr: vector(1)\n",
    "-      labels:\n",
    "-        severity: none\n",
    "     - expr: process_open_fds / process_max_fds\n",
    "       record: fd_utilization\n",
    "     - alert: FdExhaustionClose\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "echo 'kubectl edit prometheusrules --namespace monitoring kp-exporter-kube-controller-manager'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```diff\n",
    " spec:\n",
    "   groups:\n",
    "   - name: kube-controller-manager.rules\n",
    "-    rules:\n",
    "-    - alert: K8SControllerManagerDown\n",
    "-      annotations:\n",
    "-        description: There is no running K8S controller manager. Deployments and replication\n",
    "-          controllers are not making progress.\n",
    "-        runbook: https://coreos.com/tectonic/docs/latest/troubleshooting/controller-recovery.html#recovering-a-controller-manager\n",
    "-        summary: Controller manager is down\n",
    "-      expr: absent(up{job=\"kube-controller-manager\"} == 1)\n",
    "-      for: 5m\n",
    "-      labels:\n",
    "-        severity: critical\n",
    "+    rules: []\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "echo 'kubectl edit prometheusrules --namespace monitoring kp-exporter-kube-scheduler'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```diff\n",
    "       labels:\n",
    "         quantile: \"0.5\"\n",
    "       record: cluster:scheduler_binding_latency_seconds:quantile\n",
    "-    - alert: K8SSchedulerDown\n",
    "-      annotations:\n",
    "-        description: There is no running K8S scheduler. New pods are not being assigned\n",
    "-          to nodes.\n",
    "-        runbook: https://coreos.com/tectonic/docs/latest/troubleshooting/controller-recovery.html#recovering-a-scheduler\n",
    "-        summary: Scheduler is down\n",
    "-      expr: absent(up{job=\"kube-scheduler\"} == 1)\n",
    "-      for: 5m\n",
    "-      labels:\n",
    "-        severity: critical\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "echo 'kubectl edit prometheusrules --namespace monitoring kp-exporter-kubernetes --namespace monitoring'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```diff\n",
    "       for: 10m\n",
    "       labels:\n",
    "         severity: critical\n",
    "-    - alert: K8SApiserverDown\n",
    "-      annotations:\n",
    "-        description: No API servers are reachable or all have disappeared from service\n",
    "-          discovery\n",
    "-        summary: No API servers are reachable\n",
    "-      expr: absent(up{job=\"apiserver\"} == 1)\n",
    "-      for: 20m\n",
    "-      labels:\n",
    "-        severity: critical\n",
    "     - alert: K8sCertificateExpirationNotice\n",
    "       annotations:\n",
    "         description: Kubernetes API Certificate is expiring soon (less than 7 days)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### confirm prometheus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "echo 'kubectl --namespace monitoring port-forward $(kubectl get pod --namespace monitoring -l prometheus=kube-prometheus -l app=prometheus -o template --template \"{{(index .items 0).metadata.name}}\") 9090:9090'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdg-open http://localhost:9090"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. confirm Prometheus\n",
    "    * no `Target` is down.\n",
    "    * no `Alert` is fired."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setup grafana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "echo 'kubectl --namespace monitoring port-forward $(kubectl get pod --namespace monitoring -l app=kp-grafana -o template --template \"{{(index .items 0).metadata.name}}\") 3000:3000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdg-open http://localhost:3000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. login grafana\n",
    "    * At the first, a admin user (`admin`/`admin`) is available.\n",
    "2. show `Configuration -> Data Sources -> prometheus`\n",
    "3. change `URL` from `http://kp:9090` to **`http://kp-prometheus:9090`**\n",
    "4. push `Save & Test`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add `persistent volume` dashboard to grafana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. import  `monitoring/dashboard_persistent_volumes.json`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## start Elasticsearch, fluentd and Kibana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### start Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl apply -f logging/elasticsearch-azure.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get statefulsets --namespace monitoring -l k8s-app=elasticsearch-logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example)\n",
    "```\n",
    "NAME                    DESIRED   CURRENT   AGE\n",
    "elasticsearch-logging   2         2         3m\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get pods --namespace monitoring -l k8s-app=elasticsearch-logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example)\n",
    "```\n",
    "NAME                      READY     STATUS    RESTARTS   AGE\n",
    "elasticsearch-logging-0   1/1       Running   0          4m\n",
    "elasticsearch-logging-1   1/1       Running   0          2m\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get services --namespace monitoring -l k8s-app=elasticsearch-logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example)\n",
    "```\n",
    "NAME                    TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)    AGE\n",
    "elasticsearch-logging   ClusterIP   10.0.80.88   <none>        9200/TCP   4m\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get persistentvolumeclaims -n monitoring -l k8s-app=elasticsearch-logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example)\n",
    "```\n",
    "NAME                                            STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS      AGE\n",
    "elasticsearch-logging-elasticsearch-logging-0   Bound     pvc-238139db-b014-11e8-b618-066567bdfa8c   64Gi       RWO            managed-premium   4m\n",
    "elasticsearch-logging-elasticsearch-logging-1   Bound     pvc-70ca5ec3-b014-11e8-b618-066567bdfa8c   64Gi       RWO            managed-premium   2m\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl exec -it elasticsearch-logging-0 --namespace monitoring -- curl -H \"Content-Type: application/json\" -X PUT http://elasticsearch-logging:9200/_cluster/settings -d '{\"transient\": {\"cluster.routing.allocation.enable\":\"all\"}}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### start fluentd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl apply -f logging/fluentd-es-configmap.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl apply -f logging/fluentd-es-ds.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get daemonsets --namespace monitoring -l k8s-app=fluentd-es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example)\n",
    "```\n",
    "NAME                DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE\n",
    "fluentd-es-v2.2.0   4         4         4       4            4           <none>          53s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get pods --namespace monitoring -l k8s-app=fluentd-es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example)\n",
    "```\n",
    "NAME                      READY   STATUS    RESTARTS   AGE\n",
    "fluentd-es-v2.2.0-8sv45   1/1     Running   0          1m\n",
    "fluentd-es-v2.2.0-96ghs   1/1     Running   0          1m\n",
    "fluentd-es-v2.2.0-cjhtc   1/1     Running   0          1m\n",
    "fluentd-es-v2.2.0-djzff   1/1     Running   0          1m\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### start Kibana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl apply -f logging/kibana.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get pods --namespace monitoring -l k8s-app=kibana-logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example)\n",
    "```\n",
    "NAME                              READY     STATUS    RESTARTS   AGE\n",
    "kibana-logging-7444956bf8-stnfm   1/1       Running   0          1m\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### start curator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl apply -f logging/curator-configmap.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl apply -f logging/curator-cronjob.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get cronjobs --namespace monitoring -l k8s-app=elasticsearch-curator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example)\n",
    "```\n",
    "NAME                    SCHEDULE     SUSPEND   ACTIVE    LAST SCHEDULE   AGE\n",
    "elasticsearch-curator   0 18 * * *   False     0         <none>          7s\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setup Kibana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "echo 'kubectl --namespace monitoring port-forward $(kubectl get pod -l k8s-app=kibana-logging --namespace monitoring -o template --template \"{{(index .items 0).metadata.name}}\") 5601:5601'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdg-open http://localhost:5601/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. show `Management -> Index Patterns`\n",
    "2. set `logstash-*` as Index Pattern, and push `Next step`\n",
    "3. set `@timestamp` as Time Filter field name, and push `Create index pattern`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "echo 'kubectl --namespace monitoring port-forward $(kubectl get pod --namespace monitoring -l app=kp-grafana -o template --template \"{{(index .items 0).metadata.name}}\") 3000:3000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdg-open http://localhost:3000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add `elasticsearch` dashboard to grafana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. add a new Data Source\n",
    "    * Name: `elasticsearch`\n",
    "    * Type: `Elasticsearch`\n",
    "    * URL: `http://elasticsearch-logging:9200`\n",
    "    * Access: `Server(Default)`\n",
    "    * Index name: `logstash-*`\n",
    "    * Time field name: `@timestamp`\n",
    "    * Version: `5.6+`\n",
    "2. import `monitoring/dashboard_elasticsearch.json`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
